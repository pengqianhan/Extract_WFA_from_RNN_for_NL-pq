{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb63f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 20321\n",
      "data number: 22808\n",
      "Model and dataset ready. Use time:6.8\n",
      "Transitions ready. Use time:164.4\n",
      "weighted_filling & linear_regularization : 81.68%, 280s\n",
      "                  linear_regularization\n",
      "weighted_filling                  81.68\n",
      "Evaluation done.\n",
      "new transition count ready. Use time:115.6\n",
      "weighted_filling & linear_regularization : 82.08%, 277s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 1: \n",
      "                  linear_regularization\n",
      "weighted_filling                  82.08\n",
      "new transition count ready. Use time:113.2\n",
      "weighted_filling & linear_regularization : 82.13%, 253s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 2: \n",
      "                  linear_regularization\n",
      "weighted_filling                  82.13\n",
      "Workflow done. Use time:1210.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "import torchtext\n",
    "import scipy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 4)\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from device import dev\n",
    "from path import Path, Log_Path\n",
    "from dataset import dataset\n",
    "from model import RNN\n",
    "from preprocess import get_transitions, add_transitions\n",
    "\n",
    "from util import get_matrices\n",
    "from util import blank_filling, identical_filling, empirical_filling, weighted_filling, near_filling,uniform_filling\n",
    "from util import none_regularization, linear_regularization, strong_linear_regularization\n",
    "\n",
    "from synonym import get_synonym\n",
    "\n",
    "from evaluation import evaluation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # select the dataset. options: 'news', 'toxic'\n",
    "    DATASET = 'news'\n",
    "\n",
    "    # select the clusters number\n",
    "    CLUSTER = 40\n",
    "\n",
    "    # select the completion and regularization tactics\n",
    "    COMPLETION = [weighted_filling]\n",
    "    REGULARIZATION = [linear_regularization]\n",
    "\n",
    "    # select the iteration times of using synonym to augmenting dataset\n",
    "    NUM_EPOCHS = 2\n",
    "    REPLACE_RATE = 0.4\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    start_time = time.time()\n",
    "    # load model and dataset\n",
    "    train_dataset = dataset(DATASET, True)\n",
    "    test_dataset = dataset(DATASET, False)\n",
    "    model = torch.load(Path+DATASET+'_model.pth')\n",
    "    model.eval()\n",
    "    vocab_num = len(train_dataset.vocab)\n",
    "    state_num = CLUSTER + 1\n",
    "    print(f'vocab: {vocab_num}')\n",
    "    print(f'data number: {len(train_dataset.int_data)}')\n",
    "    print(f'Model and dataset ready. Use time:{time.time()-start_time:.1f}')\n",
    "\n",
    "    current_time = time.time()\n",
    "    # get rnn prediction in test set\n",
    "    rnn_prediction_container = []\n",
    "    for idx, data in enumerate(test_dataset.int_data):\n",
    "        # remove 0 at the end\n",
    "        while len(data) > 1 and data[-1] == 0:\n",
    "            data = data[0:len(data)-1]\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "        model.clear_output_sequence()\n",
    "        _ = model(data)\n",
    "        runtime_predict = model.runtime_predict()\n",
    "        runtime_data = []\n",
    "        for step_data in runtime_predict:\n",
    "            step_data = step_data.flatten().detach()\n",
    "            runtime_prediction = F.softmax(step_data,dim=0)\n",
    "            runtime_data.append(runtime_prediction.reshape(1, -1))\n",
    "        runtime_data = torch.concat(runtime_data, dim=0)\n",
    "        rnn_prediction = torch.argmax(runtime_data[-1])\n",
    "        rnn_prediction_container.append(rnn_prediction)\n",
    "\n",
    "    transition_count, kmeans, state_weightes, all_prediction_container = get_transitions(model, train_dataset, CLUSTER)\n",
    "    print(f'Transitions ready. Use time:{time.time()-current_time:.1f}')\n",
    "\n",
    "    # generate state distance\n",
    "    state_distance = torch.zeros((state_num, state_num),device=dev())\n",
    "    for p in range(state_num):\n",
    "        for q in range(state_num):\n",
    "            diff = state_weightes[p] - state_weightes[q]\n",
    "            state_distance[p, q] = (diff * diff).sum()\n",
    "    state_distance = torch.exp(state_distance)\n",
    "    state_distance = 1 / state_distance\n",
    "    \n",
    "\n",
    "    result = np.zeros((len(COMPLETION), len(REGULARIZATION)))\n",
    "    completion_names = [c.__name__ for c in COMPLETION]\n",
    "    regularization_names = [r.__name__ for r in REGULARIZATION]\n",
    "    for i, completion in enumerate(COMPLETION):\n",
    "        for j, regularization in enumerate(REGULARIZATION):\n",
    "            current_time = time.time()\n",
    "\n",
    "            transition_matrices = get_matrices(transition_count, state_distance, completion, regularization)\n",
    "            correct_rate = evaluation(test_dataset, transition_matrices, state_weightes, rnn_prediction_container)\n",
    "            result[i,j] = round(correct_rate*100, 2)\n",
    "            print(f'{completion.__name__} & {regularization.__name__} : {round(correct_rate*100, 2)}%, {time.time() - current_time:.0f}s')\n",
    "            \n",
    "    result = pd.DataFrame(result, columns=regularization_names, index=completion_names)\n",
    "    print(result)\n",
    "    print(f'Evaluation done.')\n",
    "    \n",
    "    all_synonym = torch.load(Path+DATASET+'_synonym.pth')\n",
    "    '''\n",
    "    all_synonym is a tensor with size (vocab_num, m),\n",
    "    where m is the number of synonym for each word.\n",
    "    The [i,j]-th item of all_synonym indicates the j-th synonym of i-th word.\n",
    "    If some word doe NOT have synonym, the i-th row will be filled with -1.\n",
    "    '''\n",
    "    current_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        current_time = time.time()\n",
    "        all_data = []\n",
    "        for id, data in enumerate(train_dataset.int_data):\n",
    "            # remove 0 at the end\n",
    "            while len(data) > 1 and data[-1] == 0:\n",
    "                data = data[0:len(data)-1]\n",
    "            #ori_data = data.clone()\n",
    "            for idx, word in enumerate(data):\n",
    "                if random.random() < REPLACE_RATE and word < (vocab_num/5):\n",
    "                    i = random.randint(1, 4)\n",
    "                    if all_synonym[word, 0].item() != -1:\n",
    "                        data[idx] = all_synonym[word, i].item()\n",
    "                elif random.random() < DROPOUT:\n",
    "                    data[idx] = 0\n",
    "            all_data.append(data)\n",
    "        transition_count = add_transitions(model,all_data,transition_count,kmeans)\n",
    "        print(f'new transition count ready. Use time:{time.time()-current_time:.1f}')\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            result = np.zeros((len(COMPLETION), len(REGULARIZATION)))\n",
    "            for i, completion in enumerate(COMPLETION):\n",
    "                for j, regularization in enumerate(REGULARIZATION):\n",
    "                    current_time = time.time()\n",
    "\n",
    "                    transition_matrices = get_matrices(transition_count, state_distance, completion, regularization)\n",
    "                    correct_rate = evaluation(test_dataset, transition_matrices, state_weightes, rnn_prediction_container)\n",
    "                    result[i,j] = round(correct_rate*100, 2)  \n",
    "                    print(f'{completion.__name__} & {regularization.__name__} : {round(correct_rate*100, 2)}%, {time.time() - current_time:.0f}s')\n",
    "\n",
    "            result = pd.DataFrame(result, columns=regularization_names, index=completion_names)\n",
    "            print('-'*100)          \n",
    "            print(f'epoch {epoch+1}: ')\n",
    "            print(result)\n",
    "            current_time = time.time()\n",
    "    \n",
    "    print(f'Workflow done. Use time:{time.time()-start_time:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e474770",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_prediction = kmeans.predict(all_prediction_container)\n",
    "frequencies = Counter(kmeans_prediction)\n",
    "total = sum(frequencies.values())\n",
    "for key in frequencies:\n",
    "    frequencies[key] = frequencies[key] / total \n",
    "weight_km = torch.tensor(list(dict(sorted(frequencies.items())).values())) # Frequency of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ff4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['players', 'basketball', 'celtics', 'knicks', 'bruins', 'lakers', 'yankees', 'soccer', 'coach', 'mets']\n"
     ]
    }
   ],
   "source": [
    "#Utils for part I\n",
    "def word2index(word): # input word in str format. return index.\n",
    "    return torchtext.vocab.Vocab.get_stoi(train_dataset)[word]\n",
    "    \n",
    "def index2word(index): # reverse the operation above.\n",
    "    return torchtext.vocab.Vocab.get_itos(train_dataset)[index]\n",
    "    \n",
    "def influence(word): # input an index\n",
    "    mat_word = torch.clone(transition_matrices[word][1:])\n",
    "    state_w = torch.clone(state_weightes[1:])\n",
    "    weight_km_c = torch.clone(weight_km).to('cpu')\n",
    "    mat_word_c = torch.clone(mat_word).to('cpu')\n",
    "    out_state = torch.matmul(weight_km_c,mat_word_c)[1:]\n",
    "    for i in range (len(state_w)):\n",
    "        state_w[i] = state_w[i] * ( out_state[i] - weight_km_c[i] ) \n",
    "    return torch.sum(state_w,dim = 0) # Influence vector.\n",
    "\n",
    "def topk_influence(goal_class = 0,k = 10): # Return top_k influence word list on goal_class.\n",
    "    inf_state = []\n",
    "    for i in range(len(transition_matrices)):\n",
    "        inf_state.append(influence(i)[goal_class])\n",
    "    topk_lst = heapq.nlargest(k, range(len(inf_state)), inf_state.__getitem__)\n",
    "    for i in range(len(topk_lst)):\n",
    "        topk_lst[i] = index2word(topk_lst[i])\n",
    "    return topk_lst\n",
    "\n",
    "# Test:\n",
    "print(topk_influence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd4ce535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\n"
     ]
    }
   ],
   "source": [
    "#Utils for part II\n",
    "def diff_word(w1,w2,mode = '2-norm'): # mode chosen from 'kl' and '2-norm'\n",
    "    w1_m = torch.clone(transition_matrices[w1])\n",
    "    w2_m = torch.clone(transition_matrices[w2])\n",
    "    if mode == '2-norm':\n",
    "        return torch.sum((w1_m - w2_m)**2)\n",
    "    elif mode == 'kl':\n",
    "        w1_m = w1_m.to('cpu')[1:]\n",
    "        w2_m = w2_m.to('cpu')[1:]\n",
    "        unif_s = torch.clone(weight_km).to('cpu')\n",
    "        sD_1 = torch.matmul(unif_s,w1_m)[1:]\n",
    "        sD_2 = torch.matmul(unif_s,w2_m)[1:]\n",
    "        state_w1 = torch.clone(state_weightes[1:])\n",
    "        state_w2 = torch.clone(state_weightes[1:])\n",
    "        for i in range (len(state_w1)):\n",
    "            state_w1[i] = state_w1[i] * sD_1[i]\n",
    "            state_w2[i] = state_w2[i] * sD_2[i]\n",
    "        P_1 = torch.sum(state_w1,dim = 0)\n",
    "        P_2 = torch.sum(state_w2,dim = 0)\n",
    "        return scipy.stats.entropy(P_1.cpu(),P_2.cpu())\n",
    "    \n",
    "def get_nearest(word,mode = '2-norm'):\n",
    "    goal_w = word2index(word)\n",
    "    idx = 0\n",
    "    dist = 1e6\n",
    "    for word in range(2000):\n",
    "        if (diff_word(goal_w,word,mode) < dist) and (word != goal_w):\n",
    "            idx = word\n",
    "            dist = diff_word(goal_w,word,mode)\n",
    "    return index2word(idx)\n",
    "\n",
    "# Test:\n",
    "print(get_nearest('basketball','kl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "933b85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yankees\n"
     ]
    }
   ],
   "source": [
    "# word2vec process\n",
    "from sklearn.decomposition import PCA\n",
    "def word2vec(tensor_in):\n",
    "    tensor_lst = torch.clone(tensor_in).to('cpu')\n",
    "    new_lst = []\n",
    "    for i in range(len(tensor_lst)):\n",
    "        new_lst.append(torch.flatten(tensor_lst[i]).numpy())\n",
    "    pca = PCA(n_components=2)\n",
    "    tensor_reduced = pca.fit_transform(new_lst)\n",
    "    return torch.from_numpy(tensor_reduced)\n",
    "\n",
    "def diff_word_w2v(w1,w2,mode = '2-norm'): # mode chosen from 'kl' and '2-norm'\n",
    "    w1_m = torch.clone(w2v[w1])\n",
    "    w2_m = torch.clone(w2v[w2])\n",
    "    if mode == '2-norm':\n",
    "        return torch.sum((w1_m - w2_m)**2)\n",
    "\n",
    "    \n",
    "def get_nearest_w2v(word,mode = '2-norm'):\n",
    "    goal_w = word2index(word)\n",
    "    idx = 0\n",
    "    dist = 1e6\n",
    "    for word in range(2000):\n",
    "        if (diff_word_(goal_w,word,mode) < dist) and (word != goal_w):\n",
    "            idx = word\n",
    "            dist = diff_word_(goal_w,word,mode)\n",
    "    return index2word(idx)\n",
    "\n",
    "# Test:\n",
    "# w2v = word2vec(transition_matrices)\n",
    "# print(get_nearest_w2v('nfl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
